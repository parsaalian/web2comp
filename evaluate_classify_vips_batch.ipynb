{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f5a24a-7d7e-4e3a-a9e8-8065a275d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebcc767c-22f5-4c89-a548-ecf49276718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from visca.browser import (\n",
    "    create_driver,\n",
    "    ensure_page_loaded,\n",
    "    capture_full_page_screenshot\n",
    ")\n",
    "from visca.element_extractor import (\n",
    "    extract_elements_from_driver,\n",
    "    save_elements\n",
    ")\n",
    "from visca.dedup import deduplicate_screenshots\n",
    "from visca.virtual_node import (\n",
    "    build_dom_tree,\n",
    "    VirtualNode\n",
    ")\n",
    "from visca.segment import (\n",
    "    tag_multiset,\n",
    "    jaccard_distance,\n",
    "    subtree_size,\n",
    "    calculate_psi_avg,\n",
    "    calculate_psi_sum,\n",
    "    gather_instances,\n",
    "    ascii_tree,\n",
    ")\n",
    "\n",
    "from visca.llm.gemini import create_model\n",
    "from visca.prompts import (\n",
    "    PAGE_CONTEXT_EXTRACTION_SYSTEM_PROMPT,\n",
    "    CLASSIFICATION_AND_CONTEXT_PROMPT,\n",
    "    # CONTEXTUAL_DESCRIPTION_PROMPT,\n",
    "    COMPONENT_GENERATION_PROMPT,\n",
    "    # LIST_COMPONENT_GENERATION_PROMPT\n",
    ")\n",
    "from visca.llm_processing import (\n",
    "    _get_ancestor_context,\n",
    "    classify_and_describe_candidates,\n",
    "    transform_candidate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2f7ed-16d0-4681-9df7-334af0da12e9",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc845557-f1fc-43ad-ba5c-7688e1c6c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "from typing import Tuple\n",
    "import json, textwrap, time, io\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from PIL import Image   # only if you want to display inline in a notebook\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.remote.webelement import WebElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3cbe1-9d26-4bf6-9410-b50aba11b651",
   "metadata": {},
   "source": [
    "# DOM Element Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a431da66-cbdb-4fca-91f7-463d1d046c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extraction(url: str, out_dir: str):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")  # Hide scrollbars to avoid affecting layout\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")  # Force known scale factor\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.page_load_strategy = \"eager\" # <- Eager loading \n",
    "\n",
    "    chrome_path = ChromeDriverManager().install()\n",
    "    if \"THIRD_PARTY_NOTICES.chromedriver\" in chrome_path:\n",
    "        chrome_path = chrome_path.replace(\"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\")\n",
    "    os.chmod(chrome_path, 755)\n",
    "\n",
    "    driver = Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=chrome_options\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "        \n",
    "    dom_elements = extract_elements_from_driver(driver)\n",
    "\n",
    "    dom_elements_with_screenshot = save_elements(\n",
    "    driver=driver,\n",
    "    result_dir=out_dir,\n",
    "    dom_elements=dom_elements\n",
    "    )\n",
    "\n",
    "    dom_elements_with_screenshot = list(filter(lambda x: 'screenshot' in x, dom_elements_with_screenshot))\n",
    "    all_segments = [s for s in dom_elements_with_screenshot if s['xpath'].startswith(\"//html\") and s['screenshot'] != '']\n",
    "    print(len(all_segments))\n",
    "    # deduplicated_elements = deduplicate_screenshots(dom_elements_with_screenshot)\n",
    "\n",
    "    reduced_tree = build_dom_tree(all_segments)\n",
    "\n",
    "    return reduced_tree, driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f84824-dd70-48c7-b7f1-cec241538d43",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5e9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_segments(input_path, output_path):\n",
    "    # coerce to Path in case caller passed a str\n",
    "    input_path = Path(input_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    # Load the original JSON\n",
    "    with input_path.open('r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the \"segments\" key (or empty dict if missing)\n",
    "    segments = data.get(\"segments\", {})\n",
    "\n",
    "    # Make sure output directory exists\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Write out just the segments\n",
    "    with output_path.open('w', encoding='utf-8') as f:\n",
    "        json.dump(segments, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Segments written to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecaece51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dictionary created.\n",
      "\n",
      "All 69 result folders are ready.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "DATASET_ROOT = Path(\"evaluation/segmentation/datasets/dataset-popular\")   \n",
    "RESULTS_ROOT = Path(\"evaluation/segmentation/results\")      \n",
    "DATASET_MAPPING = DATASET_ROOT / \"mapping.txt\"              \n",
    "\n",
    "# Regex pattern to extract key/value pairs\n",
    "pattern = re.compile(r'^\\s*\"(?P<key>.*?)\"\\s*:\\s*\"(?P<value>.*?)\"\\s*,?$')\n",
    "\n",
    "# Prefix to remove from each value\n",
    "prefix_to_remove = '/opt/dataset-popular/'\n",
    "\n",
    "# Function to normalize the key by stripping protocol and trailing slash\n",
    "def normalize_key(raw_key: str) -> str:\n",
    "    parsed = urlparse(raw_key)\n",
    "    key = parsed.netloc + parsed.path\n",
    "    return key.rstrip('/')\n",
    "\n",
    "# Parse the file into a dictionary, trimming the prefix and normalizing keys\n",
    "dataset = {}\n",
    "with open(DATASET_MAPPING, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line.startswith('#') or line.startswith(':'):\n",
    "            line = line[1:].strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            raw_key = m.group('key')\n",
    "            raw_value = m.group('value')\n",
    "            # Remove the prefix if present\n",
    "            if raw_value.startswith(prefix_to_remove):\n",
    "                raw_value = raw_value[len(prefix_to_remove):]\n",
    "            # Normalize the key\n",
    "            key = normalize_key(raw_key)\n",
    "            dataset[key] = raw_value\n",
    "\n",
    "print(\"Dataset dictionary created.\")\n",
    "\n",
    "for site_name, site_paths in dataset.items():            \n",
    "    (RESULTS_ROOT / site_name).mkdir(parents=True, exist_ok=True)\n",
    "    # print(f\"Created/verified: {RESULTS_ROOT/site_name}\")\n",
    "\n",
    "print(f\"\\nAll {len(dataset.keys())} result folders are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258ea709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:///Users/parsaalian/Desktop/Projects/research/auto-assert/evaluation/segmentation/datasets/dataset-popular/www.mayoclinic.com/www.mayoclinic.com/index.dom.html...\n",
      "Element screenshots saved to evaluation/segmentation/results/www.mayoclinic.com/vips\n",
      "174\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'evaluation/segmentation/results/www.mayoclinic.com/vips/vips_output_1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m pDoc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     59\u001b[0m node, driver \u001b[38;5;241m=\u001b[39m extraction(url, \u001b[38;5;28mdir\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43msplit_segments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRESULTS_ROOT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mwebpage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/vips/vips_output_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpDoc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/segmentation_vips_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpDoc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n\u001b[1;32m     64\u001b[0m screenshot \u001b[38;5;241m=\u001b[39m capture_full_page_screenshot(driver)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36msplit_segments\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m output_path \u001b[38;5;241m=\u001b[39m Path(output_path)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load the original JSON\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43minput_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Extract the \"segments\" key (or empty dict if missing)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1012\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'evaluation/segmentation/results/www.mayoclinic.com/vips/vips_output_1.json'"
     ]
    }
   ],
   "source": [
    "RESULTS_ROOT = Path(\"evaluation/segmentation/results\")      \n",
    "TARGET_HTML = [\n",
    "    # \"abcnews.go.com\",\n",
    "    # # \"eu.real.com\",\n",
    "    # \"speedtest.net\",\n",
    "    # \"www.aaas.org\",\n",
    "    # \"www.alistapart.com\",\n",
    "    # \"www.alz.org\",\n",
    "    # \"www.amnesty.org\",\n",
    "    # \"www.break.com\",\n",
    "    # \"www.cato.org\",\n",
    "    # \"www.cpsc.gov\",\n",
    "    # \"www.drugs.com\",\n",
    "    # \"www.economist.com\",\n",
    "    # \"www.euronews.com\",\n",
    "    # \"www.factcheck.org\",\n",
    "    # \"www.fodors.com\",\n",
    "    # \"www.fda.gov\",\n",
    "    # \"www.foxnews.com\",\n",
    "    # \"www.freetranslation.com\",\n",
    "    # \"www.geocaching.com\",\n",
    "    # \"www.gnu.org\",\n",
    "    # \"www.house.gov\",\n",
    "    # \"www.irs.gov\",\n",
    "    # \"www.kbb.com\",\n",
    "    # \"www.lonelyplanet.com\",\n",
    "\n",
    "    \"www.mayoclinic.com\",\n",
    "    # \"www.medicare.gov\",\n",
    "    # \"www.nature.com\",\n",
    "    # \"www.nbcnews.com\",\n",
    "    # \"www.nih.gov\",\n",
    "    # \"www.nlm.nih.gov\",\n",
    "    # \"www.noaa.gov\",\n",
    "    # \"www.openoffice.org\",\n",
    "    # \"www.scientificamerican.com\",\n",
    "    # \"www.senate.gov\",\n",
    "    # \"www.toastmasters.org\",\n",
    "    # \"www.toyota.com\",\n",
    "    # \"www.treehugger.com\",\n",
    "    # \"www.voanews.com\",\n",
    "    # \"www.w3.org\",\n",
    "    # \"www.weather.gov\",\n",
    "    # \"www.webstandards.org\",\n",
    "    # # \"www.whitehouse.gov\",\n",
    "    # \"www.wired.com\",\n",
    "    # \"www.worldbank.org\",\n",
    "    # \"www.wto.org\"\n",
    "]\n",
    "\n",
    "for webpage, html_path in dataset.items():\n",
    "    if webpage in TARGET_HTML:\n",
    "\n",
    "        # Change URL to Absolute Route of URL for Dataset Popular\n",
    "        url = f\"file:///Users/parsaalian/Desktop/Projects/research/auto-assert/evaluation/segmentation/datasets/dataset-popular/{webpage}/{webpage}/index.dom.html\"\n",
    "        dir = f\"{RESULTS_ROOT}/{webpage}/vips\"\n",
    "        pDoc = 1\n",
    "\n",
    "        node, driver = extraction(url, dir)\n",
    "\n",
    "        split_segments(f\"{RESULTS_ROOT}/{webpage}/vips/vips_output_{pDoc}.json\", f\"{dir}/segmentation_vips_{pDoc}.json\")\n",
    "\n",
    "        # Visualization\n",
    "        screenshot = capture_full_page_screenshot(driver)\n",
    "        screenshot.save(f'{dir}/screenshot.png')\n",
    "\n",
    "        # Page Classification\n",
    "        page_context_model = create_model(PAGE_CONTEXT_EXTRACTION_SYSTEM_PROMPT)\n",
    "        page_context = page_context_model(file=f'{dir}/screenshot.png').text\n",
    "        print(page_context)\n",
    "\n",
    "        # Segmentation Classification\n",
    "        classification_model = create_model(\n",
    "        CLASSIFICATION_AND_CONTEXT_PROMPT,\n",
    "        settings={\n",
    "            'temperature': 0\n",
    "        }\n",
    "        )\n",
    "\n",
    "        MEMORY = {}\n",
    "\n",
    "        classified_tree, run_log = classify_and_describe_candidates(\n",
    "            root=node,\n",
    "            classification_model=classification_model,\n",
    "            page_context=page_context,\n",
    "            memory=MEMORY,\n",
    "            segment_json_path= f\"{dir}/segmentation_vips_{pDoc}.json\"\n",
    "        )\n",
    "\n",
    "\n",
    "        # write them out\n",
    "        with open(f'{dir}/run_log_{pDoc}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(run_log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        with open(f'{dir}/llm_evaluation_{pDoc}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(classification_model.stats, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(\"Wrote run_log.json and llm_evaluation.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bc339-b860-47d8-bd76-29882a03f498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
