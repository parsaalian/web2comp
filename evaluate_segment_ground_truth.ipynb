{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f5a24a-7d7e-4e3a-a9e8-8065a275d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebcc767c-22f5-4c89-a548-ecf49276718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visca.browser import (\n",
    "    create_driver,\n",
    "    ensure_page_loaded,\n",
    "    capture_full_page_screenshot\n",
    ")\n",
    "from visca.element_extractor import (\n",
    "    extract_elements_from_driver,\n",
    "    save_elements\n",
    ")\n",
    "from visca.dedup import deduplicate_screenshots\n",
    "from visca.virtual_node import (\n",
    "    VirtualNode,\n",
    "    build_dom_tree\n",
    ")\n",
    "from visca.segment import (\n",
    "    tag_multiset,\n",
    "    jaccard_distance,\n",
    "    subtree_size,\n",
    "    calculate_psi_avg,\n",
    "    calculate_psi_sum,\n",
    "    gather_instances,\n",
    "    ascii_tree,\n",
    ")\n",
    "# from visca.llm.gemini import create_model\n",
    "# from visca.prompts import (\n",
    "#     PAGE_CONTEXT_EXTRACTION_SYSTEM_PROMPT,\n",
    "#     CLASSIFICATION_PROMPT,\n",
    "#     # CONTEXTUAL_DESCRIPTION_PROMPT,\n",
    "#     COMPONENT_GENERATION_PROMPT,\n",
    "#     LIST_COMPONENT_GENERATION_PROMPT\n",
    "# )\n",
    "# from visca.llm_processing import (\n",
    "#     _get_ancestor_context,\n",
    "#     classify_and_describe_candidates,\n",
    "#     # extract_candidates_context,\n",
    "#     transform_candidate\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2f7ed-16d0-4681-9df7-334af0da12e9",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc845557-f1fc-43ad-ba5c-7688e1c6c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.remote.webelement import WebElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3cbe1-9d26-4bf6-9410-b50aba11b651",
   "metadata": {},
   "source": [
    "# DOM Element Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a431da66-cbdb-4fca-91f7-463d1d046c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extraction(url: str, out_dir: str):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")  # Hide scrollbars to avoid affecting layout\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")  # Force known scale factor\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.page_load_strategy = \"eager\" # <- Eager loading \n",
    "\n",
    "    chrome_path = ChromeDriverManager().install()\n",
    "    if \"THIRD_PARTY_NOTICES.chromedriver\" in chrome_path:\n",
    "        chrome_path = chrome_path.replace(\"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\")\n",
    "    os.chmod(chrome_path, 755)\n",
    "\n",
    "    driver = Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=chrome_options\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "        \n",
    "    dom_elements = extract_elements_from_driver(driver)\n",
    "\n",
    "    dom_elements_with_screenshot = save_elements(\n",
    "    driver=driver,\n",
    "    result_dir=out_dir,\n",
    "    dom_elements=dom_elements\n",
    "    )\n",
    "\n",
    "    dom_elements_with_screenshot = list(filter(lambda x: 'screenshot' in x, dom_elements_with_screenshot))\n",
    "\n",
    "    deduplicated_elements = deduplicate_screenshots(dom_elements_with_screenshot)\n",
    "\n",
    "    reduced_tree = build_dom_tree(deduplicated_elements)\n",
    "\n",
    "    return reduced_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6bbc5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22c2f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/verified: evaluation/segmentation/results/www.alistapart.com\n",
      "Created/verified: evaluation/segmentation/results/www.economist.com\n",
      "Created/verified: evaluation/segmentation/results/www.findlaw.com\n",
      "Created/verified: evaluation/segmentation/results/www.irs.gov\n",
      "Created/verified: evaluation/segmentation/results/www.osha.gov\n",
      "Created/verified: evaluation/segmentation/results/www.geocaching.com\n",
      "Created/verified: evaluation/segmentation/results/www.break.com\n",
      "Created/verified: evaluation/segmentation/results/www.openoffice.org\n",
      "Created/verified: evaluation/segmentation/results/www.spiegel.de\n",
      "Created/verified: evaluation/segmentation/results/www.nlm.nih.gov\n",
      "Created/verified: evaluation/segmentation/results/speedtest.net\n",
      "Created/verified: evaluation/segmentation/results/www.who.int\n",
      "Created/verified: evaluation/segmentation/results/www.reuters.com\n",
      "Created/verified: evaluation/segmentation/results/www.freetranslation.com\n",
      "Created/verified: evaluation/segmentation/results/www.heritage.org\n",
      "Created/verified: evaluation/segmentation/results/www.scientificamerican.com\n",
      "Created/verified: evaluation/segmentation/results/www.voanews.com\n",
      "Created/verified: evaluation/segmentation/results/www.mayoclinic.com\n",
      "Created/verified: evaluation/segmentation/results/www.wired.com\n",
      "Created/verified: evaluation/segmentation/results/www.foxnews.com\n",
      "Created/verified: evaluation/segmentation/results/www.nbcnews.com\n",
      "Created/verified: evaluation/segmentation/results/www.lonelyplanet.com\n",
      "Created/verified: evaluation/segmentation/results/www.tripadvisor.com\n",
      "Created/verified: evaluation/segmentation/results/www.senate.gov\n",
      "Created/verified: evaluation/segmentation/results/www.ford.com\n",
      "Created/verified: evaluation/segmentation/results/www.postsecret.com\n",
      "Created/verified: evaluation/segmentation/results/www.wto.org\n",
      "Created/verified: evaluation/segmentation/results/www.treasury.gov\n",
      "Created/verified: evaluation/segmentation/results/www.noaa.gov\n",
      "Created/verified: evaluation/segmentation/results/www.uefa.com\n",
      "Created/verified: evaluation/segmentation/results/www.nih.gov\n",
      "Created/verified: evaluation/segmentation/results/www.amnesty.org\n",
      "Created/verified: evaluation/segmentation/results/www.fifa.com\n",
      "Created/verified: evaluation/segmentation/results/www.toastmasters.org\n",
      "Created/verified: evaluation/segmentation/results/www.euronews.com\n",
      "Created/verified: evaluation/segmentation/results/www.pandora.com\n",
      "Created/verified: evaluation/segmentation/results/www.treehugger.com\n",
      "Created/verified: evaluation/segmentation/results/www.cato.org\n",
      "Created/verified: evaluation/segmentation/results/www.fda.gov\n",
      "Created/verified: evaluation/segmentation/results/www.toyota.com\n",
      "Created/verified: evaluation/segmentation/results/www.rhymezone.com\n",
      "Created/verified: evaluation/segmentation/results/www.drugs.com\n",
      "Created/verified: evaluation/segmentation/results/www.weather.gov\n",
      "Created/verified: evaluation/segmentation/results/www.kbb.com\n",
      "Created/verified: evaluation/segmentation/results/www.medicare.gov\n",
      "Created/verified: evaluation/segmentation/results/www.cpsc.gov\n",
      "Created/verified: evaluation/segmentation/results/www.aaas.org\n",
      "Created/verified: evaluation/segmentation/results/www.journalregister.com\n",
      "Created/verified: evaluation/segmentation/results/eu.real.com\n",
      "Created/verified: evaluation/segmentation/results/www.worldbank.org\n",
      "Created/verified: evaluation/segmentation/results/www.edmunds.com\n",
      "Created/verified: evaluation/segmentation/results/www.fodors.com\n",
      "Created/verified: evaluation/segmentation/results/www.bing.com\n",
      "Created/verified: evaluation/segmentation/results/www.factcheck.org\n",
      "Created/verified: evaluation/segmentation/results/www.un.org\n",
      "Created/verified: evaluation/segmentation/results/www.nature.com\n",
      "Created/verified: evaluation/segmentation/results/www.w3.org\n",
      "Created/verified: evaluation/segmentation/results/www.house.gov\n",
      "Created/verified: evaluation/segmentation/results/www.adobe.com\n",
      "Created/verified: evaluation/segmentation/results/abcnews.go.com\n",
      "Created/verified: evaluation/segmentation/results/www.alz.org\n",
      "Created/verified: evaluation/segmentation/results/www.gnu.org\n",
      "Created/verified: evaluation/segmentation/results/www.healthfinder.gov\n",
      "Created/verified: evaluation/segmentation/results/news.google.com\n",
      "Created/verified: evaluation/segmentation/results/www.whitehouse.gov\n",
      "Created/verified: evaluation/segmentation/results/www.ama-assn.org\n",
      "Created/verified: evaluation/segmentation/results/www.sciencemag.org\n",
      "Created/verified: evaluation/segmentation/results/www.google.com\n",
      "Created/verified: evaluation/segmentation/results/www.mcclatchydc.com\n",
      "Created/verified: evaluation/segmentation/results/www.vistaprint.nl\n",
      "Created/verified: evaluation/segmentation/results/www.webstandards.org\n",
      "\n",
      "All result folders are ready.\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = Path(\"evaluation/segmentation/datasets/dataset-popular\")   # where the sites live\n",
    "RESULTS_ROOT = Path(\"evaluation/segmentation/results\")                    # where we’ll mirror them\n",
    "\n",
    "URL = 'file:////Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/eu.real.com/eu.real.com/index.blocks.html'\n",
    "DIR = 'evaluation/segmentation/results/eu.real.com/ground-truth'\n",
    "\n",
    "html_paths = {}\n",
    "\n",
    "try:\n",
    "    site_dirs = [p for p in DATASET_ROOT.expanduser().iterdir() if p.is_dir()]\n",
    "except (FileNotFoundError, PermissionError, OSError) as e:\n",
    "    raise SystemExit(f\"Cannot read dataset directory: {e}\")\n",
    "\n",
    "for site_path in site_dirs:\n",
    "    site_name = site_path.name                 \n",
    "    (RESULTS_ROOT / site_name).mkdir(exist_ok=True)\n",
    "    html_paths[site_name] = f\"{site_name}/{site_name}/index.blocks.html\"\n",
    "    print(f\"Created/verified: {RESULTS_ROOT/site_name}\")\n",
    "\n",
    "print(\"\\nAll result folders are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665e9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def load_elements(path: str | Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Read the flat element list produced by your extractor.\"\"\"\n",
    "    with open(path, encoding=\"utf-8\") as fh:\n",
    "        return json.load(fh)\n",
    "\n",
    "\n",
    "def is_descendant(child_xpath: str, ancestor_xpath: str) -> bool:\n",
    "    \"\"\"True iff *child_xpath* is strictly inside *ancestor_xpath*.\"\"\"\n",
    "    return child_xpath.startswith(ancestor_xpath + \"/\")\n",
    "\n",
    "\n",
    "def build_segments(elements: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Group elements by gt_dataBlock; for each segment root collect its leaves.\n",
    "    \"\"\"\n",
    "    # 1. roots = elements that **define** a segment\n",
    "    roots = [el for el in elements if el.get(\"gt_dataBlock\") is not None]\n",
    "\n",
    "    # 2. For quick membership tests we keep the full list in memory\n",
    "    segments_out = {}\n",
    "\n",
    "    for root in roots:\n",
    "        root_xpath = root[\"xpath\"]\n",
    "\n",
    "        # ── all descendants (depth ≥ 1) ──────────────────────────────\n",
    "        descendants = [el for el in elements\n",
    "                       if is_descendant(el[\"xpath\"], root_xpath)]\n",
    "\n",
    "        # ── a leaf has *no* descendant of its own inside the segment ─\n",
    "        leaves = []\n",
    "        for cand in descendants:\n",
    "            cand_xpath = cand[\"xpath\"]\n",
    "            has_child = any(\n",
    "                is_descendant(other[\"xpath\"], cand_xpath)\n",
    "                for other in descendants\n",
    "                if other is not cand\n",
    "            )\n",
    "            if not has_child:\n",
    "                leaves.append(cand_xpath)\n",
    "\n",
    "        # ── record ───────────────────────────────────────────────────\n",
    "        segments_out[root_xpath] = {\n",
    "            \"count\": len(leaves),\n",
    "            \"dataBlock\": root[\"gt_dataBlock\"],\n",
    "            \"dataBlockType\": root[\"gt_dataBlockType\"],\n",
    "            \"leaves\": leaves,\n",
    "        }\n",
    "\n",
    "    return segments_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cda3b8e",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572ee67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, textwrap, time, io\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from PIL import Image   # only if you want to display inline in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f68f2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_parent_bboxes(\n",
    "    url: str,\n",
    "    seg_json_path: str,\n",
    "    out_json_path: str = \"output.json\",\n",
    "    delay: float = 5.0,         # allow SPA hydration etc.\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an “auto-assert” segmentation file that contains one polygon\n",
    "    (= the left/top/right/bottom edges) for every **parent** segment\n",
    "    found in *seg_json_path*.\n",
    "\n",
    "    Output schema\n",
    "    -------------\n",
    "    {\n",
    "    \"id\": \"<file name you chose>\",\n",
    "    \"height\": <full-page CSS px>,\n",
    "    \"width\":  <full-page CSS px>,\n",
    "    \"number_of_segments\": <int>,\n",
    "    \"segmentations\": {\n",
    "        \"auto-assert\": [\n",
    "        [ [ [ [x, y], [x, y], [x, y], [x, y] ] ] ],   # segment-1 polygon\n",
    "        ...\n",
    "        ]\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # ───────────────────────────────────\n",
    "    # 1)  Launch Chrome & open page\n",
    "    # ───────────────────────────────────\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")\n",
    "    chrome_options.page_load_strategy = \"eager\"\n",
    "\n",
    "    chromedriver_path = ChromeDriverManager().install()\n",
    "    if chromedriver_path.endswith(\"THIRD_PARTY_NOTICES.chromedriver\"):\n",
    "        chromedriver_path = chromedriver_path.replace(\n",
    "            \"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\"\n",
    "        )\n",
    "    os.chmod(chromedriver_path, 0o755)\n",
    "\n",
    "    driver = Chrome(service=Service(chromedriver_path), options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # let scripts, fonts, etc. settle\n",
    "    time.sleep(delay)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "    # page dimensions in CSS px (same technique you used)\n",
    "    width = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollWidth, document.body.scrollWidth);\"\n",
    "    )\n",
    "    height = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight);\"\n",
    "    )\n",
    "    driver.set_window_size(width, height)\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 2)  Load segmentation file\n",
    "    # ───────────────────────────────────\n",
    "    with open(seg_json_path, encoding=\"utf-8\") as f:\n",
    "        seg_raw = json.load(f)           # parent_xpath → {count, leaves, …}\n",
    "\n",
    "    parent_xpaths = list(seg_raw.keys())\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 3)  Grab each parent’s bounding box\n",
    "    # ───────────────────────────────────\n",
    "    def get_box(xp):\n",
    "        # JavaScript helper returns dict {left, top, right, bottom}\n",
    "        return driver.execute_script(\n",
    "            textwrap.dedent(\n",
    "                \"\"\"\n",
    "                const xp = arguments[0];\n",
    "                const el = document.evaluate(\n",
    "                    xp, document, null,\n",
    "                    XPathResult.FIRST_ORDERED_NODE_TYPE, null\n",
    "                ).singleNodeValue;\n",
    "                if (!el) return null;\n",
    "                const r = el.getBoundingClientRect();\n",
    "                return {\n",
    "                left:   Math.round(r.left  + window.scrollX),\n",
    "                top:    Math.round(r.top   + window.scrollY),\n",
    "                right:  Math.round(r.right + window.scrollX),\n",
    "                bottom: Math.round(r.bottom+ window.scrollY)\n",
    "                };\n",
    "                \"\"\"\n",
    "            ),\n",
    "            xp,\n",
    "        )\n",
    "\n",
    "    polygons = []\n",
    "    for xp in parent_xpaths:\n",
    "        box = get_box(xp)\n",
    "        if box is None:\n",
    "            # element disappeared; skip gracefully\n",
    "            continue\n",
    "\n",
    "        # Clock-wise polygon: TL → BL → BR → TR\n",
    "        poly = [\n",
    "            [box[\"left\"],  box[\"top\"]],\n",
    "            [box[\"left\"],  box[\"bottom\"]],\n",
    "            [box[\"right\"], box[\"bottom\"]],\n",
    "            [box[\"right\"], box[\"top\"]],\n",
    "        ]\n",
    "        polygons.append([[ [ poly ] ]])   # → [[[ [x,y] … ]]] nesting like your example\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 4)  Assemble & save result JSON\n",
    "    # ───────────────────────────────────\n",
    "    out = {\n",
    "        \"id\": Path(out_json_path).name,\n",
    "        \"height\": height,\n",
    "        \"width\":  width,\n",
    "        \"number_of_segments\": len(polygons),\n",
    "        \"segmentations\": {\n",
    "            \"auto-assert\": polygons\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Path(out_json_path).write_text(json.dumps(out, indent=2))\n",
    "    print(f\"Wrote {len(polygons)} segments → {out_json_path}\")\n",
    "\n",
    "    return out_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57c2eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def screenshot_segments(\n",
    "    url: str,\n",
    "    seg_json_path: str,\n",
    "    out_dir: str = \"results\",\n",
    "    fname: str = \"segmented.png\",\n",
    "    delay: float = 5,              # wait for SPA hydration etc.\n",
    "):\n",
    "    \"\"\"Load segmentation JSON, overlay parent/leaf boxes in the browser,\n",
    "       and write a PNG screenshot to *out_dir/fname*.\n",
    "    \"\"\"\n",
    "    # ───────────────────────────────────\n",
    "    # 1)  Launch Chrome\n",
    "    # ───────────────────────────────────\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    # chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")  # Hide scrollbars to avoid affecting layout\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")  # Force known scale factor\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.page_load_strategy = \"eager\" # <- Eager loading \n",
    "\n",
    "    chrome_path = ChromeDriverManager().install()\n",
    "    if \"THIRD_PARTY_NOTICES.chromedriver\" in chrome_path:\n",
    "        chrome_path = chrome_path.replace(\"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\")\n",
    "    os.chmod(chrome_path, 755)\n",
    "\n",
    "    driver = Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=chrome_options\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "\n",
    "    w = driver.execute_script(\"return document.body.scrollWidth\")\n",
    "    h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.set_window_size(w, h)\n",
    "\n",
    "    time.sleep(delay)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 2)  Load your segmentation\n",
    "    # ───────────────────────────────────\n",
    "    with open(seg_json_path, encoding=\"utf-8\") as f:\n",
    "        seg = json.load(f)           # dict[parent_xpath] → {\"count\":…, \"leaves\":[…]}\n",
    "\n",
    "    # Full scrollable page size (CSS px) so the screenshot isn’t clipped\n",
    "    page_w = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollWidth, document.body.scrollWidth);\")\n",
    "    page_h = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight);\")\n",
    "    driver.set_window_size(page_w, page_h)\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 3)  Build & inject the overlay script\n",
    "    # ───────────────────────────────────\n",
    "    overlay_js = textwrap.dedent(f\"\"\"\n",
    "  (function drawSegmentOverlays(segments) {{\n",
    "    const node = (xp) => document.evaluate(\n",
    "      xp, document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null\n",
    "    ).singleNodeValue;\n",
    "\n",
    "    /* ---------- overlay cosmetics ---------- */\n",
    "    const style = document.createElement('style');\n",
    "    style.textContent = `\n",
    "      .overlay-parent,\n",
    "      .overlay-leaf {{\n",
    "        position:absolute;\n",
    "        z-index:2147483647;\n",
    "        pointer-events:none;\n",
    "      }}\n",
    "      /* parent → translucent RED with bold outline */\n",
    "      .overlay-parent {{\n",
    "        background:rgba(255,0,0,.25);\n",
    "        outline:2px solid red;\n",
    "      }}\n",
    "      /* leaf → translucent GREEN, **no outline**  */\n",
    "      .overlay-leaf {{\n",
    "        background:rgba(0,255,0,.25);\n",
    "      }}`;\n",
    "    document.head.appendChild(style);\n",
    "\n",
    "    /**\n",
    "     * Draw a rectangle; if inset>0, shrink it on every side\n",
    "     * so the parent’s red outline isn’t hidden.\n",
    "     */\n",
    "    const draw = (rect, cls, inset = 0) => {{\n",
    "      const d = document.createElement('div');\n",
    "      d.className = cls;\n",
    "      d.style.left   = (rect.left  + window.scrollX + inset) + 'px';\n",
    "      d.style.top    = (rect.top   + window.scrollY + inset) + 'px';\n",
    "      d.style.width  = Math.max(0, rect.width  - inset*2) + 'px';\n",
    "      d.style.height = Math.max(0, rect.height - inset*2) + 'px';\n",
    "      document.body.appendChild(d);\n",
    "    }};\n",
    "\n",
    "    Object.entries(segments).forEach(([parentXP, info]) => {{\n",
    "      const p = node(parentXP);\n",
    "      if (p) draw(p.getBoundingClientRect(), 'overlay-parent', 0);   // no inset\n",
    "      (info.leaves || []).forEach(leafXP => {{\n",
    "        const l = node(leafXP);\n",
    "        if (l) draw(l.getBoundingClientRect(), 'overlay-leaf', 1);  // 1-px inset\n",
    "      }});\n",
    "    }});\n",
    "  }})(JSON.parse({json.dumps(json.dumps(seg))}));\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "    driver.execute_script(overlay_js)\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 4)  Screenshot\n",
    "    # ───────────────────────────────────\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    png = driver.get_screenshot_as_png()\n",
    "    with open(Path(out_dir) / fname, \"wb\") as fh:\n",
    "        fh.write(png)\n",
    "\n",
    "    # Optional inline preview (e.g. in a Jupyter notebook)\n",
    "    # display(Image.open(io.BytesIO(png)))\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Screenshot written → {Path(out_dir)/fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a62d85",
   "metadata": {},
   "source": [
    "# Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32eea7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dictionary created.\n",
      "\n",
      "All 70 result folders are ready.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "DATASET_ROOT = Path(\"evaluation/segmentation/datasets/dataset-popular\")   \n",
    "RESULTS_ROOT = Path(\"evaluation/segmentation/results\")      \n",
    "DATASET_MAPPING = DATASET_ROOT / \"mapping.txt\"              \n",
    "\n",
    "# Regex pattern to extract key/value pairs\n",
    "pattern = re.compile(r'^\\s*\"(?P<key>.*?)\"\\s*:\\s*\"(?P<value>.*?)\"\\s*,?$')\n",
    "\n",
    "# Prefix to remove from each value\n",
    "prefix_to_remove = '/opt/dataset-popular/'\n",
    "\n",
    "# Function to normalize the key by stripping protocol and trailing slash\n",
    "def normalize_key(raw_key: str) -> str:\n",
    "    parsed = urlparse(raw_key)\n",
    "    key = parsed.netloc + parsed.path\n",
    "    return key.rstrip('/')\n",
    "\n",
    "# Parse the file into a dictionary, trimming the prefix and normalizing keys\n",
    "dataset = {}\n",
    "with open(DATASET_MAPPING, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if line.startswith('#') or line.startswith(':'):\n",
    "            line = line[1:].strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "        m = pattern.match(line)\n",
    "        if m:\n",
    "            raw_key = m.group('key')\n",
    "            raw_value = m.group('value')\n",
    "            # Remove the prefix if present\n",
    "            if raw_value.startswith(prefix_to_remove):\n",
    "                raw_value = raw_value[len(prefix_to_remove):]\n",
    "            # Normalize the key\n",
    "            key = normalize_key(raw_key)\n",
    "            dataset[key] = raw_value\n",
    "\n",
    "print(\"Dataset dictionary created.\")\n",
    "\n",
    "for site_name, site_paths in dataset.items():            \n",
    "    (RESULTS_ROOT / site_name).mkdir(parents=True, exist_ok=True)\n",
    "    # print(f\"Created/verified: {RESULTS_ROOT/site_name}\")\n",
    "\n",
    "print(f\"\\nAll {len(dataset.keys())} result folders are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eba76df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 'eu.real.com' → evaluation/segmentation/results/eu.real.com/ground-truth\n",
      "Processing file:///Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/eu.real.com/eu.real.com/index.blocks.html...\n",
      "Element screenshots saved to evaluation/segmentation/results/eu.real.com/ground-truth\n",
      "Computing image hashes for 140 segments...\n",
      "Found 6 exact hash duplicates\n",
      "Found 11 padding duplicates\n",
      "Removing 17 duplicate screenshots...\n",
      "Deduplication complete. Kept 123 of 140 segments.\n",
      "Screenshot written → evaluation/segmentation/results/eu.real.com/ground-truth/boxed.gt.png\n",
      "Wrote 11 segments → evaluation/segmentation/results/eu.real.com/ground-truth/segmentation_bbox_gt.json\n",
      "\n",
      "Ground Truth Segmentation ran on 1 webpages.\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "\n",
    "for webpage, html_path in dataset.items():\n",
    "    if webpage == \"eu.real.com\":\n",
    "        url = f\"file:///Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/{webpage}/{webpage}/index.blocks.html\"\n",
    "        out_dir = RESULTS_ROOT / webpage / \"ground-truth\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"\\nProcessing {webpage!r} → {out_dir}\")\n",
    "\n",
    "        # 1) extraction\n",
    "        try:\n",
    "            node = extraction(url, str(out_dir))\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] extraction failed for {webpage!r}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 2) build and save mapping\n",
    "        try:\n",
    "            json_path = out_dir / \"segments.json\"\n",
    "            elements = load_elements(str(json_path))\n",
    "            mapping = build_segments(elements)\n",
    "            output_path = out_dir / \"segmentation_xpath_gt.json\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "                json.dump(mapping, fh, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] building/saving segments failed for {webpage!r}: {e}\")\n",
    "\n",
    "        # 3) screenshot\n",
    "        try:\n",
    "            screenshot_segments(\n",
    "                url=url,\n",
    "                seg_json_path=str(out_dir / \"segmentation_xpath_gt.json\"),\n",
    "                out_dir=str(out_dir),\n",
    "                fname=\"boxed.gt.png\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] screenshot_segments failed for {webpage!r}: {e}\")\n",
    "\n",
    "        # 4) export bboxes\n",
    "        try:\n",
    "            export_parent_bboxes(\n",
    "                url=url,\n",
    "                seg_json_path=str(out_dir / \"segmentation_xpath_gt.json\"),\n",
    "                out_json_path=str(out_dir / \"segmentation_bbox_gt.json\"),\n",
    "                delay=5.0,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] export_parent_bboxes failed for {webpage!r}: {e}\")\n",
    "\n",
    "        success += 1\n",
    "\n",
    "print(f\"\\nGround Truth Segmentation ran on {success} webpages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a810a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_assert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
