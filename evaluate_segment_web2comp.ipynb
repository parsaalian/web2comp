{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f5a24a-7d7e-4e3a-a9e8-8065a275d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebcc767c-22f5-4c89-a548-ecf49276718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visca.browser import (\n",
    "    create_driver,\n",
    "    ensure_page_loaded,\n",
    "    capture_full_page_screenshot\n",
    ")\n",
    "from visca.element_extractor import (\n",
    "    extract_elements_from_driver,\n",
    "    save_elements\n",
    ")\n",
    "from visca.dedup import deduplicate_screenshots\n",
    "from visca.virtual_node import (\n",
    "    VirtualNode,\n",
    "    build_dom_tree\n",
    ")\n",
    "from visca.segment import (\n",
    "    tag_multiset,\n",
    "    jaccard_distance,\n",
    "    subtree_size,\n",
    "    calculate_psi_avg,\n",
    "    calculate_psi_sum,\n",
    "    gather_instances,\n",
    "    ascii_tree,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2f7ed-16d0-4681-9df7-334af0da12e9",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc845557-f1fc-43ad-ba5c-7688e1c6c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "from selenium.webdriver.remote.webelement import WebElement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3cbe1-9d26-4bf6-9410-b50aba11b651",
   "metadata": {},
   "source": [
    "# DOM Element Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a431da66-cbdb-4fca-91f7-463d1d046c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extraction(url: str, out_dir: str):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")  # Hide scrollbars to avoid affecting layout\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")  # Force known scale factor\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.page_load_strategy = \"eager\" # <- Eager loading \n",
    "\n",
    "    chrome_path = ChromeDriverManager().install()\n",
    "    if \"THIRD_PARTY_NOTICES.chromedriver\" in chrome_path:\n",
    "        chrome_path = chrome_path.replace(\"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\")\n",
    "    os.chmod(chrome_path, 755)\n",
    "\n",
    "    driver = Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=chrome_options\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "        \n",
    "    dom_elements = extract_elements_from_driver(driver)\n",
    "\n",
    "    dom_elements_with_screenshot = save_elements(\n",
    "    driver=driver,\n",
    "    result_dir=out_dir,\n",
    "    dom_elements=dom_elements\n",
    "    )\n",
    "\n",
    "    dom_elements_with_screenshot = list(filter(lambda x: 'screenshot' in x, dom_elements_with_screenshot))\n",
    "\n",
    "    deduplicated_elements = deduplicate_screenshots(dom_elements_with_screenshot)\n",
    "\n",
    "    reduced_tree = build_dom_tree(deduplicated_elements)\n",
    "\n",
    "    return reduced_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f84824-dd70-48c7-b7f1-cec241538d43",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e568331",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effd665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(reduced_tree: VirtualNode, out_dir: str):\n",
    "    calculate_psi_sum(reduced_tree)                           # new size logic in use\n",
    "    instances = gather_instances(reduced_tree)\n",
    "\n",
    "    print(\"Number of Segments: \", len(instances.keys()))\n",
    "\n",
    "    for xp, size in instances.items():\n",
    "        print(f\"{xp:<60}  subtree-nodes = {size}\")\n",
    "\n",
    "    with open(f'{out_dir}/segments.json','r',encoding='utf-8') as f:\n",
    "        leaves = json.load(f)\n",
    "\n",
    "    # group leaf XPaths under each root‑XPath\n",
    "    instance_details = {}\n",
    "    for root_xpath, count in instances.items():\n",
    "        \n",
    "        grouped = [\n",
    "        l for l in leaves\n",
    "        if l['xpath'] != root_xpath                 # not the root itself\n",
    "        and l['xpath'].startswith(root_xpath + '/') # true descendants only\n",
    "        ]\n",
    "\n",
    "        instance_details[root_xpath] = {\n",
    "            'count':  count,\n",
    "            'leaves': [l['xpath'] for l in grouped]\n",
    "        }\n",
    "\n",
    "    with open(f\"{out_dir}/segmentation_xpath_aa.json\", 'w', encoding='utf-8') as out:\n",
    "        json.dump(instance_details, out, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce43ffa-0567-4d4b-b747-edcc6388bcd0",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df44dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, textwrap, time, io\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from PIL import Image   # only if you want to display inline in a notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bbbad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_parent_bboxes(\n",
    "    url: str,\n",
    "    seg_json_path: str,\n",
    "    out_json_path: str = \"output.json\",\n",
    "    delay: float = 5.0,         # allow SPA hydration etc.\n",
    "):\n",
    "    \"\"\"\n",
    "    Build an “auto-assert” segmentation file that contains one polygon\n",
    "    (= the left/top/right/bottom edges) for every **parent** segment\n",
    "    found in *seg_json_path*.\n",
    "\n",
    "    Output schema\n",
    "    -------------\n",
    "    {\n",
    "    \"id\": \"<file name you chose>\",\n",
    "    \"height\": <full-page CSS px>,\n",
    "    \"width\":  <full-page CSS px>,\n",
    "    \"number_of_segments\": <int>,\n",
    "    \"segmentations\": {\n",
    "        \"auto-assert\": [\n",
    "        [ [ [ [x, y], [x, y], [x, y], [x, y] ] ] ],   # segment-1 polygon\n",
    "        ...\n",
    "        ]\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # ───────────────────────────────────\n",
    "    # 1)  Launch Chrome & open page\n",
    "    # ───────────────────────────────────\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")\n",
    "    chrome_options.page_load_strategy = \"eager\"\n",
    "\n",
    "    chromedriver_path = ChromeDriverManager().install()\n",
    "    if chromedriver_path.endswith(\"THIRD_PARTY_NOTICES.chromedriver\"):\n",
    "        chromedriver_path = chromedriver_path.replace(\n",
    "            \"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\"\n",
    "        )\n",
    "    os.chmod(chromedriver_path, 0o755)\n",
    "\n",
    "    driver = Chrome(service=Service(chromedriver_path), options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # let scripts, fonts, etc. settle\n",
    "    time.sleep(delay)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "    # page dimensions in CSS px (same technique you used)\n",
    "    width = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollWidth, document.body.scrollWidth);\"\n",
    "    )\n",
    "    height = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight);\"\n",
    "    )\n",
    "    driver.set_window_size(width, height)\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 2)  Load segmentation file\n",
    "    # ───────────────────────────────────\n",
    "    with open(seg_json_path, encoding=\"utf-8\") as f:\n",
    "        seg_raw = json.load(f)           # parent_xpath → {count, leaves, …}\n",
    "\n",
    "    parent_xpaths = list(seg_raw.keys())\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 3)  Grab each parent’s bounding box\n",
    "    # ───────────────────────────────────\n",
    "    def get_box(xp):\n",
    "        # JavaScript helper returns dict {left, top, right, bottom}\n",
    "        return driver.execute_script(\n",
    "            textwrap.dedent(\n",
    "                \"\"\"\n",
    "                const xp = arguments[0];\n",
    "                const el = document.evaluate(\n",
    "                    xp, document, null,\n",
    "                    XPathResult.FIRST_ORDERED_NODE_TYPE, null\n",
    "                ).singleNodeValue;\n",
    "                if (!el) return null;\n",
    "                const r = el.getBoundingClientRect();\n",
    "                return {\n",
    "                left:   Math.round(r.left  + window.scrollX),\n",
    "                top:    Math.round(r.top   + window.scrollY),\n",
    "                right:  Math.round(r.right + window.scrollX),\n",
    "                bottom: Math.round(r.bottom+ window.scrollY)\n",
    "                };\n",
    "                \"\"\"\n",
    "            ),\n",
    "            xp,\n",
    "        )\n",
    "\n",
    "    polygons = []\n",
    "    for xp in parent_xpaths:\n",
    "        box = get_box(xp)\n",
    "        if box is None:\n",
    "            # element disappeared; skip gracefully\n",
    "            continue\n",
    "\n",
    "        # Clock-wise polygon: TL → BL → BR → TR\n",
    "        poly = [\n",
    "            [box[\"left\"],  box[\"top\"]],\n",
    "            [box[\"left\"],  box[\"bottom\"]],\n",
    "            [box[\"right\"], box[\"bottom\"]],\n",
    "            [box[\"right\"], box[\"top\"]],\n",
    "        ]\n",
    "        polygons.append([[ [ poly ] ]])   # → [[[ [x,y] … ]]] nesting like your example\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 4)  Assemble & save result JSON\n",
    "    # ───────────────────────────────────\n",
    "    out = {\n",
    "        \"id\": Path(out_json_path).name,\n",
    "        \"height\": height,\n",
    "        \"width\":  width,\n",
    "        \"number_of_segments\": len(polygons),\n",
    "        \"segmentations\": {\n",
    "            \"auto-assert\": polygons\n",
    "        }\n",
    "    }\n",
    "\n",
    "    Path(out_json_path).write_text(json.dumps(out, indent=2))\n",
    "    print(f\"Wrote {len(polygons)} segments → {out_json_path}\")\n",
    "\n",
    "    return out_json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df77fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def screenshot_segments(\n",
    "    url: str,\n",
    "    seg_json_path: str,\n",
    "    out_dir: str = \"results\",\n",
    "    fname: str = \"segmented.png\",\n",
    "    delay: float = 5,              # wait for SPA hydration etc.\n",
    "):\n",
    "    \"\"\"Load segmentation JSON, overlay parent/leaf boxes in the browser,\n",
    "       and write a PNG screenshot to *out_dir/fname*.\n",
    "    \"\"\"\n",
    "    # ───────────────────────────────────\n",
    "    # 1)  Launch Chrome\n",
    "    # ───────────────────────────────────\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    # chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--hide-scrollbars\")  # Hide scrollbars to avoid affecting layout\n",
    "    chrome_options.add_argument(\"--force-device-scale-factor=1\")  # Force known scale factor\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.page_load_strategy = \"eager\" # <- Eager loading \n",
    "\n",
    "    chrome_path = ChromeDriverManager().install()\n",
    "    if \"THIRD_PARTY_NOTICES.chromedriver\" in chrome_path:\n",
    "        chrome_path = chrome_path.replace(\"THIRD_PARTY_NOTICES.chromedriver\", \"chromedriver\")\n",
    "    os.chmod(chrome_path, 755)\n",
    "\n",
    "    driver = Chrome(\n",
    "        service=Service(ChromeDriverManager().install()),\n",
    "        options=chrome_options\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "\n",
    "    w = driver.execute_script(\"return document.body.scrollWidth\")\n",
    "    h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.set_window_size(w, h)\n",
    "\n",
    "    time.sleep(delay)\n",
    "    driver.execute_script(\"window.stop();\")\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 2)  Load your segmentation\n",
    "    # ───────────────────────────────────\n",
    "    with open(seg_json_path, encoding=\"utf-8\") as f:\n",
    "        seg = json.load(f)           # dict[parent_xpath] → {\"count\":…, \"leaves\":[…]}\n",
    "\n",
    "    # Full scrollable page size (CSS px) so the screenshot isn’t clipped\n",
    "    page_w = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollWidth, document.body.scrollWidth);\")\n",
    "    page_h = driver.execute_script(\n",
    "        \"return Math.max(document.documentElement.scrollHeight, document.body.scrollHeight);\")\n",
    "    driver.set_window_size(page_w, page_h)\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 3)  Build & inject the overlay script\n",
    "    # ───────────────────────────────────\n",
    "    overlay_js = textwrap.dedent(f\"\"\"\n",
    "  (function drawSegmentOverlays(segments) {{\n",
    "    const node = (xp) => document.evaluate(\n",
    "      xp, document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null\n",
    "    ).singleNodeValue;\n",
    "\n",
    "    /* ---------- overlay cosmetics ---------- */\n",
    "    const style = document.createElement('style');\n",
    "    style.textContent = `\n",
    "      .overlay-parent,\n",
    "      .overlay-leaf {{\n",
    "        position:absolute;\n",
    "        z-index:2147483647;\n",
    "        pointer-events:none;\n",
    "      }}\n",
    "      /* parent → translucent RED with bold outline */\n",
    "      .overlay-parent {{\n",
    "        background:rgba(255,0,0,.25);\n",
    "        outline:2px solid red;\n",
    "      }}\n",
    "      /* leaf → translucent GREEN, **no outline**  */\n",
    "      .overlay-leaf {{\n",
    "        background:rgba(0,255,0,.25);\n",
    "      }}`;\n",
    "    document.head.appendChild(style);\n",
    "\n",
    "    /**\n",
    "     * Draw a rectangle; if inset>0, shrink it on every side\n",
    "     * so the parent’s red outline isn’t hidden.\n",
    "     */\n",
    "    const draw = (rect, cls, inset = 0) => {{\n",
    "      const d = document.createElement('div');\n",
    "      d.className = cls;\n",
    "      d.style.left   = (rect.left  + window.scrollX + inset) + 'px';\n",
    "      d.style.top    = (rect.top   + window.scrollY + inset) + 'px';\n",
    "      d.style.width  = Math.max(0, rect.width  - inset*2) + 'px';\n",
    "      d.style.height = Math.max(0, rect.height - inset*2) + 'px';\n",
    "      document.body.appendChild(d);\n",
    "    }};\n",
    "\n",
    "    Object.entries(segments).forEach(([parentXP, info]) => {{\n",
    "      const p = node(parentXP);\n",
    "      if (p) draw(p.getBoundingClientRect(), 'overlay-parent', 0);   // no inset\n",
    "      (info.leaves || []).forEach(leafXP => {{\n",
    "        const l = node(leafXP);\n",
    "        if (l) draw(l.getBoundingClientRect(), 'overlay-leaf', 1);  // 1-px inset\n",
    "      }});\n",
    "    }});\n",
    "  }})(JSON.parse({json.dumps(json.dumps(seg))}));\n",
    "\"\"\")\n",
    "\n",
    "    driver.execute_script(overlay_js)\n",
    "\n",
    "    # ───────────────────────────────────\n",
    "    # 4)  Screenshot\n",
    "    # ───────────────────────────────────\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    png = driver.get_screenshot_as_png()\n",
    "    with open(Path(out_dir) / fname, \"wb\") as fh:\n",
    "        fh.write(png)\n",
    "\n",
    "    # Optional inline preview (e.g. in a Jupyter notebook)\n",
    "    # display(Image.open(io.BytesIO(png)))\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Screenshot written → {Path(out_dir)/fname}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc72134",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b46cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── status_cache.py ───────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "import json, tempfile, os, logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "CACHE_PATH = Path(\"run_status.json\")\n",
    "\n",
    "def _now() -> str:\n",
    "    \"\"\"Return current time in ISO-8601 with timezone, e.g. 2025-05-26T01:47:03-07:00\"\"\"\n",
    "    return datetime.now().astimezone().isoformat(timespec=\"seconds\")\n",
    "\n",
    "def load_cache() -> Dict[str, Any]:\n",
    "    if CACHE_PATH.is_file():\n",
    "        try:\n",
    "            return json.loads(CACHE_PATH.read_text())\n",
    "        except Exception as exc:               # corrupted cache? start fresh but keep a backup\n",
    "            logging.warning(\"Status cache unreadable – starting new file (%s)\", exc)\n",
    "            CACHE_PATH.rename(CACHE_PATH.with_suffix(\".bak\"))\n",
    "    return {}\n",
    "\n",
    "def save_cache(cache: Dict[str, Any]) -> None:\n",
    "    # atomic write: write to tmp file and rename over the old one\n",
    "    tmp = CACHE_PATH.with_suffix(\".tmp\")\n",
    "    tmp.write_text(json.dumps(cache, indent=2))\n",
    "    tmp.replace(CACHE_PATH)\n",
    "\n",
    "def mark_result(cache: Dict[str, Any],\n",
    "                site: str,\n",
    "                doc: int | str,\n",
    "                ok: bool,\n",
    "                message: str | None = None) -> None:\n",
    "    entry = cache.setdefault(site, {})           # one sub-object per site\n",
    "    entry[str(doc)] = {\n",
    "        \"status\"    : \"success\" if ok else \"failure\",\n",
    "        \"timestamp\" : _now(),\n",
    "        **({\"error\": message} if not ok else {})\n",
    "    }\n",
    "    save_cache(cache)                            # flush immediately\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f4c05",
   "metadata": {},
   "source": [
    "# Individual Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa93ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/eu.real.com/eu.real.com/index.dom.html evaluation/segmentation/results/eu.real.com/auto-assert\n",
      "Processing file:///Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/eu.real.com/eu.real.com/index.dom.html...\n",
      "Element screenshots saved to evaluation/segmentation/results/eu.real.com/auto-assert\n",
      "Computing image hashes for 140 segments...\n",
      "Found 6 exact hash duplicates\n",
      "Found 44 padding duplicates\n",
      "Removing 50 duplicate screenshots...\n",
      "Deduplication complete. Kept 90 of 140 segments.\n",
      "Number of Segments:  21\n",
      "//html[1]/body[1]/div[1]/div[1]                               subtree-nodes = 1\n",
      "//html[1]/body[1]/div[1]/div[2]                               subtree-nodes = 8\n",
      "//html[1]/body[1]/div[1]/div[3]                               subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/div[1]/div[1]/ul[1]/li[1]/a[1]  subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/div[1]/div[1]/ul[1]/li[2]/a[1]  subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/div[1]/div[1]/ul[1]/li[3]/a[1]  subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/p[1]/a[1]                     subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/div[1]/div[1]                 subtree-nodes = 6\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/h3[1]                  subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/p[1]                   subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/div[1]/ul[1]/li[1]     subtree-nodes = 5\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/div[1]/ul[1]/li[2]     subtree-nodes = 5\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/div[1]/ul[1]/li[3]     subtree-nodes = 4\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/div[1]/ul[1]/li[4]     subtree-nodes = 4\n",
      "//html[1]/body[1]/div[2]/div[1]/div[3]/p[2]                   subtree-nodes = 1\n",
      "//html[1]/body[1]/div[2]/div[1]/ul[1]                         subtree-nodes = 1\n",
      "//html[1]/body[1]/div[3]/div[1]/p[1]                          subtree-nodes = 1\n",
      "//html[1]/body[1]/div[3]/div[1]/ul[1]/li[1]                   subtree-nodes = 8\n",
      "//html[1]/body[1]/div[3]/div[1]/ul[1]/li[2]                   subtree-nodes = 16\n",
      "//html[1]/body[1]/div[3]/div[1]/ul[1]/li[3]                   subtree-nodes = 8\n",
      "//html[1]/body[1]/div[3]/div[1]/ul[1]/li[4]                   subtree-nodes = 6\n",
      "Screenshot written → evaluation/segmentation/results/eu.real.com/auto-assert/auto_assert_boxed.png\n",
      "Wrote 21 segments → evaluation/segmentation/results/eu.real.com/auto-assert/segmentation_bbox_aa.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'evaluation/segmentation/results/eu.real.com/auto-assert/segmentation_bbox_aa.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_ROOT = Path(\"evaluation/segmentation/results\")      \n",
    "\n",
    "webpage = \"eu.real.com\"\n",
    "url = f\"file:///Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/{webpage}/{webpage}/index.dom.html\"\n",
    "dir = f\"{RESULTS_ROOT}/{webpage}/auto-assert\"\n",
    "print(url, dir)\n",
    "\n",
    "node = extraction(url, dir)\n",
    "segmentation(node, dir)\n",
    "screenshot_segments(\n",
    "    url=url,\n",
    "    seg_json_path=f\"{dir}/segmentation_xpath_aa.json\",\n",
    "    out_dir=dir,\n",
    "    fname=\"auto_assert_boxed.png\",\n",
    ")\n",
    "\n",
    "export_parent_bboxes(\n",
    "    url=url,\n",
    "    seg_json_path=f\"{dir}/segmentation_xpath_aa.json\",\n",
    "    out_json_path=f\"{dir}/segmentation_bbox_aa.json\",\n",
    "    delay=5.0,   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6bbc5",
   "metadata": {},
   "source": [
    "# Batch Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4daad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import re\n",
    "# from urllib.parse import urlparse\n",
    "\n",
    "# DATASET_ROOT   = Path(\"evaluation/segmentation/datasets/dataset-popular\")\n",
    "# RESULTS_ROOT = Path(\"evaluation/segmentation/results\")   \n",
    "# DATASET_MAPPING = DATASET_ROOT / \"mapping.txt\"\n",
    "\n",
    "# pattern = re.compile(r'^\\s*\"(?P<key>.*?)\"\\s*:\\s*\"(?P<value>.*?)\"\\s*,?$')\n",
    "# prefix_to_remove = '/opt/dataset-popular/'          # path prefix inside mapping.txt\n",
    "\n",
    "# def normalize_key(raw_key: str) -> str:\n",
    "#     p = urlparse(raw_key)\n",
    "#     return (p.netloc + p.path).rstrip('/')\n",
    "\n",
    "# dataset = {}\n",
    "# with open(DATASET_MAPPING, encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue\n",
    "\n",
    "#         # strip comment markers (# or :) that head some lines\n",
    "#         if line.startswith(('#', ':')):\n",
    "#             line = line[1:].strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "\n",
    "#         m = pattern.match(line)\n",
    "#         if not m:\n",
    "#             continue\n",
    "\n",
    "#         raw_key   = m.group('key')\n",
    "#         raw_value = m.group('value')\n",
    "\n",
    "#         # trim prefix added inside the container\n",
    "#         if raw_value.startswith(prefix_to_remove):\n",
    "#             raw_value = raw_value[len(prefix_to_remove):]\n",
    "\n",
    "#         # ───────────────────────────────────────────────────────\n",
    "#         # Prefer the DOM-annotated snapshot if it exists\n",
    "#         # ───────────────────────────────────────────────────────\n",
    "#         rel_path  = Path(raw_value)                      # e.g.  www.site.com/index.html\n",
    "#         if rel_path.name == \"index.html\":\n",
    "#             dom_candidate = rel_path.with_name(\"index.dom.html\")\n",
    "#             if (DATASET_ROOT / dom_candidate).is_file():   # ← check once on disk\n",
    "#                 raw_value = str(dom_candidate)\n",
    "\n",
    "#         dataset[normalize_key(raw_key)] = raw_value\n",
    "\n",
    "# print(\"Dataset dictionary created.\")\n",
    "\n",
    "# for site_name, html_rel_path in dataset.items():\n",
    "#     (RESULTS_ROOT / site_name).mkdir(parents=True, exist_ok=True)\n",
    "#     print(f\"✓ {site_name}: {html_rel_path}\")\n",
    "\n",
    "# print(f\"\\nAll {len(dataset)} result folders are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e994ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# success = 0\n",
    "# import shutil\n",
    "# from pathlib import Path\n",
    "\n",
    "# for webpage, html_path in dataset.items():\n",
    "#     url = f\"file:///Users/martintang/Desktop/Github/auto-assert/evaluation/segmentation/datasets/dataset-popular/{html_path}\"\n",
    "#     out_dir = RESULTS_ROOT / webpage / \"auto-assert\"\n",
    "#     print(f\"\\nProcessing {webpage!r} | {html_path} → {out_dir}\")\n",
    "\n",
    "#     # if it already exists, delete it (and everything inside)\n",
    "#     if out_dir.exists():\n",
    "#         shutil.rmtree(out_dir)\n",
    "\n",
    "#     # now recreate it from scratch\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # 1) extraction\n",
    "#     try:\n",
    "#         node = extraction(url, str(out_dir))\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] extraction failed for {webpage!r}: {e}\")\n",
    "#         # skip to next page entirely\n",
    "#         continue\n",
    "\n",
    "#     # 2) segmentation\n",
    "#     try:\n",
    "#         segmentation(node, str(out_dir))\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] segmentation failed for {webpage!r}: {e}\")\n",
    "\n",
    "#     # 3) screenshot\n",
    "#     try:\n",
    "#         screenshot_segments(\n",
    "#             url=url,\n",
    "#             seg_json_path=f\"{out_dir}/segmentation_xpath_aa.json\",\n",
    "#             out_dir=str(out_dir),\n",
    "#             fname=\"segmentation_aa.png\",\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] screenshot_segments failed for {webpage!r}: {e}\")\n",
    "\n",
    "#     # 4) export bboxes\n",
    "#     try:\n",
    "#         export_parent_bboxes(\n",
    "#             url=url,\n",
    "#             seg_json_path=f\"{out_dir}/segmentation_xpath_aa.json\",\n",
    "#             out_json_path=f\"{out_dir}/segmentation_bbox_aa.json\",\n",
    "#             delay=5.0,   \n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"[WARN] export_parent_bboxes failed for {webpage!r}: {e}\")\n",
    "\n",
    "#     success += 1\n",
    "\n",
    "# print(f\"\\nAuto-Assert Segmentation ran on {success} webpages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853d86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_assert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
